You are world class polymath and strategic thinker. You excel in seeing new ways of looking at problems, looking around corners, and building insightful narratives on where to go from here and how to get there.

---

First read up all documents in Docs/Reference/* to understand what this codebase is about, why it exists, and what's built so far. Read why.md to familirize yourself with what's at stake and why you should care.

---

Science advances with explanations. We observe something and we propose causal explanations for the phenomena we observe. We have to necessarily make a leap of faith in coming up with explanations because we don't know what we don't know. Then our explnations (theories) make predictions that can be falsified. This leads to more data that needs to explanation. This is the beginning of infinity as David Deustch describes. It's the same journey we're on. We ask ourselves, why does market turn where it does? It's very similar to asking the question why does a river turn where it does. One can never truly measure the length of a riverbank because it's fractal. One has to choose a scale to even attempt to measure it. We have a similar problem statement with our market generator. So we have chosen some scales and proposed an explanation to begin the journey to infinity.

The first step on this journey is nearly complete. The swings being detected have very few false negatives and false positives as observed by this human. We can work to reduce it, but at this point we have entered diminishing returns on it. The next logical thing to do is to use empericism to advance the theory. This requires that we make falsifiable predictions, and use data to falsify and refine our theory. Crucially -- we must refine our explanation until it fits the reality we observe really well. Towards this journey, the next step is to discretize the time series data we possess. 

You're here to think about what's a good way to discretize the continuous time series data, think OHLC of ES-1m as an example. It has 6 million bars. Enough to observe patterns but nowhere enough to to even consider training any model (all data points will be fit to parameters if we tried). How do we convert this data to discrete game pieces?  

You are one of 5-6 agents who will independently work on this. Mechanically start by creating a document in Proposals/Drafts/Discretization/inprogress-UUID.md (we'll rename this in the end). You may enter planning mode, create todos, or use tools as you see fit. This will be a long running complex task to solve a hard problem. Prepare accordingly. At each step consider writing your output. down in the document so you can make clear progress.

Here's my suggestion on how to approach it:

First step: write the synthesis of a problem statement. Use reference documents and this prompt to write it as you see fit. This should include key problems to address in your document. Going back to the beginning of infinity, a well formed problem statement(s) is more than half the battle to getting better understanding of reality. We understand that that we can never fully grasp reality but we can get better asymptotically. The problem statement or the question you pose limits how close you get to reality. So, think through this carefully. 

Second step: what guiding tenets will you use to make your decisions sharper and to anchor your virtual disucssion? Good tenets anticipate and tiebreak hard tradeoffs that cannot be made with data or with what's already given. So think about potential tradeoffs you have to make to serve your problem statement. Or think about general guiding principles that you must adhere to given the nature of the project and your task. If your readers or your panel disagree with these, then that creates good discussion that we can later reflect on. [See appendix 1 about writing good tenets and use it.] 

Third step: virtually consult. Find eminent experts whose expertise and opinion will be valuable in this exercise. They can be current or past geniuses. Doesn't matter as long as you can authentically think in their voice (i.e., enough is known about their worldview and it's useful for your exercise). Personify each of these experts and have them speak in their voice and express opinions, questions, or directions on the problem statement and where the project is headed. They can be contrarians or they can be aligned. 

You do not moderate these discussions prematurely. Good discussions rarely go stragight to the point. They sometimes meander. But this meandering that may seem tangential at first may turn out to be very important at synthesis. Think of the master and the emssiary. The emissary line of thinking is direct and explicit, but it often misses holistic picture that integrates conflicting points of view without reducing any of them or turning them into explicit compromsised chimeras. That is the master's domain. Think of any meandering experts as exploring important intutive spaces that are explicitly tangential but very pertinent in a broader view. That said, there is a point to this. All experts you've chosen care about the project and want to move it meaningfully forward. That's the synthesis they all (and you) want. 

Fourth step, once you've considered all the experts' points of views, think about thier implications for the problem at hand. Use your own voice (or use an expert) and deliberate on options and tradeoffs. There can be more than one good option or it may be a synthesis. Break them down and think through them. Consider tenets to break tensions if you find any. Alternatively use the voice of experts and eminent thinkers as necessary. For example, is one expert clearly very deep in the direction you're going? If so, would they think you're applying it right or would they steer you towards something else? After you've done this analysis, you should be in a place to make your case for where we should go. Don't give the reader ingredients, but rather you should bake the cake. What's the solution you're honing in on? That should become clear once you've completed this rumination.   

Finally write the executive summary and project proposal sections at the very top. Don't bury the lede, lead with it. Use active voice direct sentences and make your case directly here. You should explain your recommended solution, why you think this is right, the alternatives you considered, and why you strongly believe this is in the best interest of the project. It may be one of the options or a combination of many. 

You may iterate on the the document if you are not satisfied that the recommendation you came up with is a high quality solution. 

Now let's bring in most important considerations to keep in mind. 

First, the key to our success here is interpretability. This may be the single most important consideration to evaluate market generator's output for fidelity with real markets. If we use a black box neural approach, we risk overfitting the admittedly small amount of data. Even at 1m interval ES data for 20 years amounts 6 million bars. The number of discrete features we extract from it will necessarily be orders of magnitude smaller. And this explanation is based on discretionary trader's view of backtesting 4+ years of ES and SPX data, so we cannot simply extend it by using other tickers. Tickers like NQ, Mag7, and BTC follow similar rules but this human hasn't extensively backtested them as much as ES and NQ. Other tickers may have completely different set of rules. That is to say, we cannot overcome the data problem by bringing in more tickers, as those require more explanatory theories. For now, start with the hypothesis that the data we have to lear rules are ES-1M OHLCV. If this proves to be strictly not enough (see notes on news later), then bring that up. making this work jsut with avaiable data has high impact on the momentum of the project.  

So, given the volume of data, any attempt to learn more than a few dozen or max of hundred tunable parameters likely results in overfitting. You're free to consult experts about this as this is consideration worth keeping on top your list. A stochastic interpretable rule based model on the other hand can be expanded endlessly without loss of fidilety. The question of whether this accurately models the market is a key question. We can start with the hypothesis that this must be so given our theory. But we must seek evidence to improve our theory or (in the worst case) falsify it as quickly as possible. 

Second, what's our ultimate goal with this generator? Key goal for us is to enable creation of a market data generator that can generate realistic market. This will be used to train models, to gamify and learn for humans etc. In this first step, we don't have to solve this whole problem, only how to discretize it. But we cannot stray from this goal and discretization is merely a step to get there. So deliberate on the bigger problem as long as you want before honing in on discretization proposal. We will be successful if we can create realistic market simulation. Evaluation criteria is unknown and you can deliberate. Expert confusion may be a decent point to start but wil be very hard to measure. Discriminator model may be easier to measure but it may be challenging to construct one that doesn't memorize real market data. Consider this carefully. 

Third, what are our game pieces in the theory as we know it right now? At its core it's valid bear and bull reference swings. We hypothesize that all market moves can be decomposed to level-to-level move in some relevant swing. Move completions are invalidations or 2x swings as defined in the north star document. This is not to say markets cannot move many levels or even transend completions. They can, and often do (for example, a move can go 3x or even 4x, but very likely this is in service of a larger move). Our aim is to look at the decomposed ES data and learn these patterns as cleanly as we can. We use rules from the north star as predictions made by this theory at the XL and L scales. We hold these strong opinions weakly, especially at smaller scales. We hypothesize that there can be multiple swings with competing levels that can introduce "battles" that are stochastically resolved. Our core assumption is that we can convert this data into OHLC by reversing discretization step. You can deliberate on this as needed to build a clearer picture. 

Fourth, we hypothesize that stochastic news prediction provides causal explanations for momentum, Volatility clustering, breakouts and breakdowns, and failed breakouts or breakdowns. Our theory is one model of causality, so we take the null hypothesis that the implied causality in north star document is false and will aim to reject it. If this is correct, then fib relationships, momentum characteristics will ideally be indistinguishable from real markets. This will require additional components named in the north star. Most important of this is a news model. This provides a stochastic generation of "news" events with polarity and intensity. We don't need to treat this as black box. One way to make this tractable for the discteization step is to just consume another data source that has CPI, FOMC, NFP, etc., high impact market events from the past decade or so. This is explicitly out of scope for the work at hand as we don't have this data set right now. But it's an important consideration so we don't make one-way door decisions that end up being expensive later. For now, we can simplify this and use predictable news dates (economic data) and known past tail events (like COVID) as proxy for actual news model. 

Fifth, self-similarity is implied but not rigid. We assume there is one set of rules for all scales that are self similar. But this is not rigid as bigger scales influence smallers scales causally allowing noisy completions. This means smaller scales may have more exceptions allowed to the rules than larger scales. As we get larger, the market structure should dominate the moves much more than stocashtic noise. Ultimately, our theory suggests even small scale "noise" has causal factors, but they're obscured from us at this level so we approximate them using larger scale rules. 

Sixth, game pieces thus far envisioned looks something like scale -> swing --> advance / retreat level --> complete / invalidate. But read extensively in this repo, have the experts explore creatively, question, and convince themselves of wha the right approach is. How to model this is entirely unclear at present, whether to model them as time series (something like language tokens) or heirarchial structures, or something else entirely. It's up to you to decide.

Seventh, this translation will necessarily be lossy. But the only loss should be in "how" the smallest move that we can resonable make is made. We can have some sense of this how such as time it took or volume (if available) but we may not know the exact path. This is acceptable and completely expected. The idea is not to preserve 100% of teh data, rather it is to biuld a model of how it operates based on our theory. 

Finalizing your document: as you end your document, consider naming. In computer science there are only two hard problems, naming, cache invalidation, and off by one errors :). Before you end, consider naming. Name your team of experts. It should be a simple one word (or max two words name). Name your proposal, again it should be something simple and elegant. Then write the very top heading. What are you telling your read. Write this deliberately and carefully. You're not writing an academic paper. You're writing a project proposal in the context of the north star and why this project exists. Write that. Then write the sub-heading or tag line. Sign the very end of the document with your name, time, and date. 

Finally, use the names you came up with and rename the document to the following format. 

Proposals/Drafts/Discretization/<HHmm:dd-mmm-yyyy-[your_team_name]_[your_proposal_name].md

----

If this is clear, begin.

-----

Appendix: “Once you make a decision, the universe conspires to make it happen.”

– Ralph Waldo Emerson, essayist & philosopher

Thinking is hard; energy intensive even. Perhaps that’s the reason we obsess about food. One estimate indicates we make over 200 food-related decisions a day. Another popular internet statistic boldly states that the average human makes 35,000 decisions a day. I’m skeptical but it illustrates a point: decision-making consumes a significant part of our day.

Part of the complexity is that each of us interprets the data related to decisions differently, consciously and unconsciously, informed by our own biases and experiences. In the workplace the added pressures of multiple priorities, career aspirations, and politics further complicates the problem. So it’s not surprising that decision-making in organisations contribute massively to delays, misunderstandings, project failures, and conflict. One officer at my previous company found that in some cases, “technology projects” deemed slow were often preceded by business decisions which dragged on for a decade or more.

I see the same with many cloud migrations and initiatives today. The cloud inherently allows teams to make decisions, try experiments, and course correct, quicker. This fails though if decisions are not being made at the same speed. Indecision and unhealthy conflict can be minimised by taking the time to create a common framework and language for making decisions. This is where tenets excel.

What is a tenet?
A tenet is an organisational belief that accelerates decision-making. Tenets clarify and align people on what is important and what, by extension, isn’t. Tenets can be applied to teams, products, services, or even whole organisations. Architects will be familiar with tenets as they are the essence of good architecture: deciding what trade-offs need to be made as I’ve discussed previously.

A good tenet concisely and clearly articulates a single idea, acting as a tie breaker in decisions. Take the following tenet:

“We optimise for speed. Speed enables us to learn quickly, pivot if needed, and to scale quickly.”

What does this mean to a decision-maker? It advocates that getting new ideas into the hands of customers to measure efficacy is more important than a hypothetical perfect solution stuck in a deployment backlog. It acknowledges that ideas might not work, and that changing direction based on learnings is healthy. In McDonald’s we adopted a similar approach to accelerate digital deployments, using the tenet “progress over perfection.” It was powerful because it made imperfection permissible, avoiding the never ending planning cycles in a futile hope of avoiding any potential problems.

Tenets are useful

To be effective a tenet needs to have an opposite which could be acceptable to an organisation. It helps prevent blindly obvious statements being turned into tenets. A tenet such as “we optimise for value” doesn’t help with decision-making. After all, would you really optimise to deliver useless features quickly? Similarly “We should not build unsecure software” is both common sense and does not have a defensible alternative.

Corporate mantras (“We will think outside of the box”) and actual decisions (“The team will only use Java”) also make for poor tenets. They don’t fulfill a key role of a tenet: to help make decisions on where time and effort should be spent. Let me illustrate this with a tenet from our Principal Engineering community:

“Exemplary Practitioner – Principal Engineers are hands-on and lead by example. We deliver artifacts that set the standard for engineering excellence, from designs to algorithms to implementations. Only by being close to the details can we earn the respect needed to be effective technical leaders.”

It’s a tenet that helps ensure principal engineers retain their engineering disciplines and are capable of deep diving into issues to help and coach their teams. Similarly, the hybrid architecture tenets we use recognise and promote the need for consistent control planes, tools, SLAs and APIs regardless of your deployment model. They promote ease of deployment, management, and support over cool but inconsistent functionality.

Tenets are durable
As I write this, there is an argument exploding on social media about whether microservices are good or bad. Aside from a plea for the CTOs involved to stop treating technology as a religious, binary debate, it illustrates how the application of tenets may change over time but the actual tenets stay relatively stable. For a product or service team, tenets should be applicable from ideation through to scaling, and beyond, into subsequent releases. A particular feature may fail, but the underlying tenets should remain true. With the current debate, using serverless to accelerate time to market for a particular service achieved the rapidity of learnings a team needed, but a different architecture was more appropriate as the service scaled massively.

Tenets create tension
The previous point also illustrates the need for high judgement when applying tenets as there will typically be tensions between tenets.  Take an example of a complementary tenet to the one promoting optimising for speed:

“We continue to drive the cost of transactions down as we scale. Optimising costs enables us to build new differentiating features.”

Do I perhaps spend more to deliver value faster, or do I focus on reducing cost? Well, it depends. There are the natural trade-offs as I’ve discussed in my previous blog post. Initially the focus might be on moving fast and learning quickly. As an application scales, this second tenet aligns us all that we want to continue to learn fast, but to do so we need to attract more customers and/or drive down the average transaction cost to free up investments. This might mean rearchitecting to or from, say, serverless or a particular database. These aren’t holy debates; they are objective considerations to achieve a mission given the current point of time, informed by your tenets.

Tenets are concise
The general rule of thumb is to have no more than 7 tenets, in descending order of priority. This helps avoid the natural temptation to try to cover every eventuality. Keeping the tenet itself and the subsequent description short forces a level of precision and discipline that helps makes them useful and meaningful even to new starters on a team. It helps eliminate weasel words and hedging that dilute the impact and the intent of each of the tenets.

Tenets promote trust
Tenets help talented teams take decisions faster and with less conflict, particularly in high judgement situations. They may reinforce an existing belief or establish a new convention such as automating undifferentiated work, or building software only where it is a competitive differentiator.

Well-written and used tenets promote trust. If I know how one of my teams is intending to make decisions through clearly articulate tenets, I’m less like to feel the need to get involved in every decision. As a leader I can spend more time ensuring the decision-making framework is right, not every single decision, leading to more trust, more speed, and less conflict.

Unless you know better ones
Any set of tenets in AWS is followed by the phrase “unless you know better ones.” It invites debate and an open mindedness on whether one or more tenets remain appropriate as an organisation or product evolves. It is itself a tension within tenets: creating tenets which are both durable and changeable.

If, like me, you believe leadership is about creating a framework in which your teams can operate with autonomy and clarity, tenets are a natural tool to help on this journey. Consistently I find that investing time in developing tenets surfaces and resolves differences of opinions, ultimately leading to a more conduicive environment to delivering value faster. Less wasted time and energy, and more time for you to really think about what you want for dinner, for pleasure not fuel.

